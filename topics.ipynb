{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic analysis of covid-19 related literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "The corpus of documents has been retrieved from [LitCovid](https://www.ncbi.nlm.nih.gov/research/coronavirus/), a curated open-resource literature of pubmed research papers related to new coronavirus disease COVID-19, and from [medRxiv and bioRxiv COVID-19 SARS-CoV-2](https://connect.biorxiv.org/relate/content/181) preprints. The text of each document, composed by the concatenation of titles and abstracts, has been tokenized, stemmed, and filtered by stopwords. Duplicate documents, due in some case to edited paper available also on preprint servers, have been removed by comparing the title string with Levenshtein distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from RISparser import readris\n",
    "import dateutil.parser as parser\n",
    "\n",
    "dois=[]\n",
    "docs = []\n",
    "titles = []\n",
    "year = []\n",
    "source = []\n",
    "title_len =[]\n",
    "with open('data/06202020.litcovid.export.ris', 'r') as bibliography_file:\n",
    "    entries = readris(bibliography_file)\n",
    "    for entry in entries:\n",
    "        dois.append(entry['doi'])\n",
    "        titles.append(entry['title'])\n",
    "        docs.append(entry['title'] +' '+ entry['abstract'])\n",
    "        source.append('pubmed')\n",
    "        year.append(entry['year'])\n",
    "        title_len.append(len(entry['title']))\n",
    "\n",
    "with open('data/biomed-covid.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for i in data['rels']:\n",
    "    #if not i['rel_doi'] in dois:\n",
    "    dois.append(i['rel_doi'])\n",
    "    titles.append(i['rel_title'])\n",
    "    docs.append(i['rel_title']+' '+i['rel_abs'])\n",
    "    source.append(i['rel_site'])\n",
    "    year.append(str(parser.parse(i['rel_date']).year))\n",
    "    title_len.append(len(i['rel_title']))\n",
    "\n",
    "print(len(docs))\n",
    "df = pd.DataFrame({'year':year,'doi':dois,'title':titles,\n",
    "                   'txt':docs,'source':source,\n",
    "                  'tlen':title_len})\n",
    "\n",
    "df['txt'] = df['txt'].str.lower()\n",
    "\n",
    "df = df[df['txt'].str.match('\\w')]\n",
    "df = df[(df['year']=='2020') | (df['year']=='2019')]\n",
    "df = df[df['tlen']<400]\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, stemming, and stop words removing\n",
    "\n",
    "The corpus text content, composed by title and abstract, has been processed to extract tokens, i.e. alphanumeric sequence of characters separated by space. Punctuation and pure numbers have been removed. Each word has then been reconducted to its root lemma to normalize the vocabulary and avoid redudant words due to morphological and inflexional endings in English. Finally common english words has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, Step, and Stop words\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import string  \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"english\")\n",
    "stops.extend(['et','al','editor','review','abstract','result','conslusion','spacing','href','html','http','title', 'stats', 'bio'])\n",
    "\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print('nltk version: ' + nltk.__version__)\n",
    "\n",
    "def rm_munnezz(tk):\n",
    "    return [w for w in tk if (w not in string.punctuation) and (not re.match(r'\\d+',w)) and (not re.match(r'^[^a-zA-Z]',w))]\n",
    "\n",
    "def uscore_intra_sym(tk):\n",
    "    return [re.sub('[^a-zA-Z\\d\\s]','_',w) for w in tk]\n",
    "\n",
    "def stem(tw):\n",
    "    return [stemmer.stem(word) for word in tw]\n",
    "\n",
    "def stop(sl):\n",
    "    return [w for w in sl if not w in stops]\n",
    "\n",
    "\n",
    "tokens = df['txt'].apply(nltk.word_tokenize)\n",
    "token_words = tokens.apply(rm_munnezz)\n",
    "token_words = token_words.apply(uscore_intra_sym)\n",
    "stemmed_list = token_words.apply(stem)\n",
    "meaningful_words = stemmed_list.apply(stop)\n",
    "\n",
    "#meaningful_words = token_words.apply(stop)\n",
    "\n",
    "df['processed'] = meaningful_words.apply(\" \".join)\n",
    "\n",
    "# save the processed corpus\n",
    "f = open('results/docs.pckl', 'wb')\n",
    "pickle.dump(df, f)\n",
    "f.close()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect duplicated with cosine similarity\n",
    "import numpy as np\n",
    "import scipy.cluster\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "f = open('results/docs.pckl', 'rb')\n",
    "df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "vectoriser = TfidfVectorizer(max_df=0.2,min_df=2)\n",
    "doc_train = vectoriser.fit_transform(df['processed'])\n",
    "print(df.shape)\n",
    "\n",
    "# remove docs with no significant terms\n",
    "w=np.max(doc_train,axis=1)\n",
    "w=w.toarray()[:,0]\n",
    "df = df[w!=0]\n",
    "\n",
    "doc_train = vectoriser.fit_transform(df['processed'])\n",
    "print(df.shape)\n",
    "\n",
    "TD = pairwise_distances(doc_train,metric='cosine',n_jobs=-1)\n",
    "np.fill_diagonal(TD,0)\n",
    "TD = squareform(TD)\n",
    "Z = linkage(TD,method='single')\n",
    "\n",
    "\n",
    "ct = hierarchy.cut_tree(Z, height=0.1)\n",
    "ct = ct.reshape(-1)\n",
    "uct = np.unique(ct)\n",
    "w = np.array([np.sum(ct==i) for i in uct])\n",
    "\n",
    "dropidx = np.array([],int)\n",
    "for ci in uct[w==2]:\n",
    "    k = np.where((ct==ci) & (df['source'].values!='pubmed'))[0]\n",
    "    dropidx = np.append(dropidx,k)\n",
    "dropidx\n",
    "\n",
    "retainidx = [i for i in list(range(df.shape[0])) if i not in dropidx]\n",
    "print(len(retainidx))\n",
    "print(df.shape[0] - len(dropidx))\n",
    "df = df.iloc[retainidx,]\n",
    "\n",
    "# save the processed corpus\n",
    "f = open('results/docs.pckl', 'wb')\n",
    "pickle.dump(df, f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model hyperparameters optimization\n",
    "\n",
    "LDA hyperparameter optimization include the search for an optimal number of topics T. A grid search has been performed within a rage of values. Perplexity has been adopted as a measure of fitness to choose the optimal number of topics. Alpha and Beta parameters have been left to 1/T, recognized as optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for optimal number of topics\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "print('sklearn version: ' + sklearn.__version__)\n",
    "\n",
    "\n",
    "f = open('results/docs.pckl', 'rb')\n",
    "df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "vectoriser = CountVectorizer(max_df=0.2,min_df=2,\n",
    "                             #max_features=10000,\n",
    "                             ngram_range=(1,1))\n",
    "doc_train = vectoriser.fit_transform(df['processed'])\n",
    "\n",
    "perplexity = []\n",
    "score = []\n",
    "tt = []\n",
    "topics = (5,10,20,22,24,26,28,29,30,32,34,36,38,40,45,50,55,60,70,80,90,100)\n",
    "for T in topics:\n",
    "    print(T)\n",
    "    # Fit LDA to the data \n",
    "    LDA = LatentDirichletAllocation(n_components = T, \n",
    "                                    learning_method='batch',\n",
    "                                    verbose=0, random_state=0,\n",
    "                                    n_jobs=25)\n",
    "    lda = LDA.fit(doc_train)\n",
    "    perplexity.append(lda.perplexity(doc_train))\n",
    "    score.append(lda.score(doc_train))\n",
    "    tt.append(T)\n",
    "    \n",
    "\n",
    "df_scores = pd.DataFrame({'T':tt,'score':score,'perplexity':perplexity})\n",
    "\n",
    "# save results on file\n",
    "f = open('results/topic_scores.pckl', 'wb')\n",
    "pickle.dump(df_scores, f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot perplexity vs number of topics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "f = open('results/topic_scores.pckl', 'rb')\n",
    "df_scores = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "df_scores.plot(kind='line',x='T',y='perplexity',grid=True,figsize=(10,5))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn topics from the corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27570, 18449)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute of words frequencies\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "f = open('results/docs.pckl', 'rb')\n",
    "df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.2,min_df=2,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(df['processed'])\n",
    "\n",
    "print(X.shape) # check shape of the document-term matrix\n",
    "\n",
    "terms_count = X.sum(axis=0)\n",
    "terms_count = terms_count / float(terms_count.max())\n",
    "dict_word_frequency = {}\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(len(terms)):\n",
    "    dict_word_frequency[terms[i]] = terms_count[0,i]    \n",
    "\n",
    "\n",
    "wcloud = WordCloud(background_color=\"white\",mask=None, max_words=1000,\\\n",
    "                        max_font_size=60,min_font_size=10,prefer_horizontal=0.9,\n",
    "                        contour_width=3,contour_color='black')\n",
    "wcloud.generate_from_frequencies(dict_word_frequency)\n",
    "\n",
    "plt.figure(figsize=(50,20))\n",
    "plt.imshow(wcloud, interpolation='bilinear')\n",
    "plt.title('Corpus top words',fontsize=50)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of topics = 36\n",
      "(27570, 36)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# get the optimal number of topics from grid search results\n",
    "f = open('results/topic_scores.pckl', 'rb')\n",
    "df_scores = pickle.load(f)\n",
    "f.close()\n",
    "n_topics = df_scores['T'].values[df_scores['perplexity'].idxmin()]\n",
    "print('Optimal number of topics = %d' % n_topics)\n",
    "model = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                    learning_method='batch',\n",
    "                                    verbose=0, random_state=0,\n",
    "                                    n_jobs=25)\n",
    "topic_matrix = model.fit_transform(X)\n",
    "\n",
    "print(topic_matrix.shape)\n",
    "\n",
    "# save document topic distributions\n",
    "f = open('results/topic_matrix.pckl', 'wb')\n",
    "pickle.dump(topic_matrix, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show topic top word clouds \n",
    "import math   \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "terms_count = 25\n",
    "numtopics = len(model.components_)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "ncol=4\n",
    "nrow = int(math.ceil(numtopics/ncol))\n",
    "fig, ax = plt.subplots(nrows=nrow, ncols=ncol,figsize=(25, 30))\n",
    "fig.suptitle('Topics identified in COVID-19 literature', fontsize=30)\n",
    "\n",
    "#Looping over lda components to get topics and their related terms with high probabilities\n",
    "for idx,topic in enumerate(model.components_):    \n",
    "    #print('Topic# ',idx)\n",
    "    abs_topic = abs(topic)\n",
    "    topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]\n",
    "    topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]\n",
    "    topic_words = []\n",
    "    for i in range(terms_count):\n",
    "        topic_words.append(topic_terms_sorted[i][0])\n",
    "    #print(','.join( word for word in topic_words))\n",
    "    #print(\"\")\n",
    "    dict_word_frequency = {}\n",
    "    \n",
    "    for i in range(terms_count):\n",
    "        dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]    \n",
    "    wcloud = WordCloud(background_color=\"white\",mask=None, max_words=100,\\\n",
    "                        max_font_size=60,min_font_size=10,prefer_horizontal=0.9,\n",
    "                        contour_width=3,contour_color='black')\n",
    "    wcloud.generate_from_frequencies(dict_word_frequency)\n",
    "    i = idx // ncol\n",
    "    j = idx % ncol\n",
    "    ax[i,j].imshow(wcloud, interpolation='bilinear')\n",
    "    ax[i,j].set_axis_off()\n",
    "    ax[i,j].set_title('#%d' % idx,fontsize=20)\n",
    "fig.savefig(\"Topics-topwords.pdf\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('results/docs.pckl', 'rb')\n",
    "df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('results/topic_matrix.pckl', 'rb')\n",
    "topic_matrix = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def topic_top_words(model, feature_names, n_top_words):\n",
    "    tword=[]\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        #message = \"Topic #%d: \" % topic_idx\n",
    "        message = \",\".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        tword.append(message)\n",
    "    return(tword)\n",
    "\n",
    "# Define helper functions\n",
    "def doc_topic(topic_matrix):\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def doc_topic2(topic_matrix):\n",
    "    keys = topic_matrix.argsort(axis=1)\n",
    "    return keys[:,-2]\n",
    "\n",
    "\n",
    "ttop = topic_top_words(model, terms, 10)\n",
    "topic = [ttop[i] for i in doc_topic(topic_matrix)]\n",
    "\n",
    "topic2 = [ttop[i] for i in doc_topic2(topic_matrix)]\n",
    "\n",
    "\n",
    "df['First topic']=topic\n",
    "df['Second topic']=topic2\n",
    "df['First topic id']=doc_topic(topic_matrix)\n",
    "df['Second topic id']=doc_topic2(topic_matrix)\n",
    "\n",
    "# entropy of each document\n",
    "e = np.sum(-topic_matrix * np.log2(topic_matrix),axis=1)\n",
    "\n",
    "df['entropy']=e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# categorie francesca\n",
    "#f = open('results/docs.pckl', 'rb')\n",
    "#df = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "frdois = ['10.1101/2020.05.12.091256','10.3390/jcm9040982']\n",
    "cls = ['network']*2\n",
    "frdois.extend(['10.1080/07391102.2020.1756411','10.1038/d41586-020-01221-y','10.1016/j.compbiomed.2020.103670'])\n",
    "cls.extend(['vaccine_serological']*3)\n",
    "frdois.extend(['10.1038/s41591-020-0944-y','10.21203/rs.3.rs-27220/v1','10.1101/2020.03.15.20033472','10.1101/2020.05.05.079194','10.1101/2020.05.05.079095','10.1038/s41421-020-0168-9','10.1101/2020.05.18.101717','10.1101/2020.05.18.100545','10.1016/j.cell.2020.05.025','10.15252/embj.20105114','10.1038/s41591-020-0901-9','10.1101/2020.05.06.081695','10.1101/2020.04.08.029769','10.1038/s41586-020-2456-9','10.1101/2020.05.20.20107813','10.1101/2020.05.20.106294','10.1016/j.cell.2020.04.026','10.1080/22221751.2020.1747363','10.1101/2020.06.22.165225','10.1101/2020.04.21.051912','10.1016/j.stem.2020.06.015','10.1101/2020.05.25.115600','10.1126/science.abc1669','10.1016/j.cell.2020.05.006','10.1101/2020.04.29.20084327','10.1016/j.cell.2020.04.004','10.1101/2020.05.02.20084673','10.1101/2020.05.06.081695','10.2139/ssrn.3564998'])\n",
    "cls.extend(['sequencing']*29)\n",
    "frdois.extend(['10.1101/2020.05.21.108506','10.1101/2020.04.09.034942','10.1101/2020.03.30.016790','10.1016/j.meegid.2020.104351'])\n",
    "cls.extend(['phylo']*4)\n",
    "frdois.extend(['10.13140/RG.2.2.20588.10886','10.1093/bib/bbw051','10.1093/bib/bbw051','10.1101/2020.03.11.986836','10.1016/j.gendis.2020.04.002'])\n",
    "cls.extend(['drug_docking']*5)\n",
    "\n",
    "print(len(cls))\n",
    "frcls = dict(zip(frdois,cls))\n",
    "\n",
    "t = [frcls.get(i,'NA') for i in df['doi']]\n",
    "df['annotation'] = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "First topic                                                               source \n",
       "antibodi,igg,serolog,igm,assay,respons,test,convalesc,plasma,neutral      biorxiv     23\n",
       "                                                                          medrxiv     94\n",
       "                                                                          pubmed     140\n",
       "cell,ace2,express,receptor,lung,immun,human,gene,enzym,may                biorxiv    151\n",
       "                                                                          medrxiv     96\n",
       "                                                                          pubmed     480\n",
       "drug,virus,vaccin,genom,viral,treatment,sequenc,effect,potenti,therapeut  biorxiv    194\n",
       "                                                                          medrxiv     42\n",
       "                                                                          pubmed     777\n",
       "predict,model,learn,score,deep,valid,perform,method,machin,imag           biorxiv      1\n",
       "                                                                          medrxiv    103\n",
       "                                                                          pubmed     163\n",
       "protein,bind,spike,structur,human,viral,sars_cov,virus,target,cell        biorxiv    369\n",
       "                                                                          medrxiv     17\n",
       "                                                                          pubmed     382\n",
       "Name: doi, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select document with low entropy and those with first topic belonging to biomarker related\n",
    "\n",
    "# topics related to biomarkers\n",
    "#ourtopicids = [33,3,4,27,23,20]\n",
    "ourtopicids = [0,1,33,27,20]\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(np.sum(df['annotation']!='NA'))\n",
    "\n",
    "t=df[df['entropy']<(-0.5*np.log2(1/n_topics))]\n",
    "\n",
    "t = t[t['First topic id'].isin(ourtopicids)] # | t['Second topic id'].isin(ourtopicids)]\n",
    "t.drop('txt', axis=1, inplace=True)\n",
    "t.drop('processed', axis=1, inplace=True)\n",
    "t['doi link'] = '=HYPERLINK(\"https://doi.org/'+t['doi'] + '\",\"'+t['doi']+'\")'\n",
    "t.to_excel('docs-filtered.xlsx',index=False)\n",
    "\n",
    "# shows the table with the number of docs in each topic category\n",
    "t.groupby(['First topic','source']).count()['doi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '0,245'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-570b87546031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Journal Categories'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Journal SJR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Journal SJR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Journal SJR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'docs-filtered2.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[1;32m   5689\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5690\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[0;32m-> 5691\u001b[0;31m                                          **kwargs)\n\u001b[0m\u001b[1;32m   5692\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                                             copy=align_copy)\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[0;32m--> 534\u001b[0;31m                             **kwargs)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;31m# TODO(extension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '0,245'"
     ]
    }
   ],
   "source": [
    "# annotate references with journal names, categories, and scimago ranking percentile\n",
    "from crossref.restful import Works\n",
    "import pandas as pd\n",
    "\n",
    "sjr = pd.read_csv('data/scimagojr 2019.csv',sep=';')\n",
    "\n",
    "\n",
    "works = Works()\n",
    "\n",
    "jj = []\n",
    "jc = []\n",
    "jsi = []\n",
    "\n",
    "for index, row in t.iterrows():\n",
    "    w = works.doi(row['doi'])\n",
    "    if w:\n",
    "        if 'container-title' in w:\n",
    "            #print(w['container-title'])\n",
    "            if len(w['container-title'])>0:\n",
    "                jj.append(w['container-title'][0])\n",
    "                cat = sjr[sjr['Title'].str.match(w['container-title'][0])]\n",
    "                if cat.shape[0]>0:\n",
    "                    jc.append(cat['Categories'].values[0])\n",
    "                    jsi.append(cat['SJR'].values[0])\n",
    "                else:\n",
    "                    jc.append('NA')\n",
    "                    jsi.append(0)\n",
    "            else:\n",
    "                jj.append('NA')\n",
    "                jc.append('NA')\n",
    "                jsi.append(0)\n",
    "        else:\n",
    "            jj.append('NA')\n",
    "            jc.append('NA')\n",
    "            jsi.append(0)\n",
    "    else:\n",
    "        jj.append('NA')\n",
    "        jc.append('NA')\n",
    "        jsi.append(0)\n",
    "\n",
    "t['Journal'] = jj\n",
    "t['Journal Categories'] = jc\n",
    "t['Journal SJR'] = jsi\n",
    "t['Journal SJR'] = t['Journal SJR'].str.replace(',','.')\n",
    "t['Journal SJR'] = t['Journal SJR'].astype(float)\n",
    "\n",
    "t.to_excel('docs-filtered2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40            0\n",
       "45            0\n",
       "66            0\n",
       "68        0,245\n",
       "83            0\n",
       "128           0\n",
       "137       1,601\n",
       "144       1,361\n",
       "147           0\n",
       "148       1,158\n",
       "149       1,158\n",
       "172      36,691\n",
       "176       6,141\n",
       "186       2,406\n",
       "221       0,755\n",
       "249       2,994\n",
       "252       1,608\n",
       "254       1,119\n",
       "261       1,633\n",
       "262       0,694\n",
       "269       2,163\n",
       "275       7,920\n",
       "276       1,649\n",
       "289       0,971\n",
       "291       3,280\n",
       "300       3,410\n",
       "318       1,185\n",
       "324           0\n",
       "334           0\n",
       "343       1,608\n",
       "          ...  \n",
       "28834         0\n",
       "28835         0\n",
       "28837         0\n",
       "28841         0\n",
       "28842         0\n",
       "28843         0\n",
       "28853         0\n",
       "28857         0\n",
       "28858         0\n",
       "28859         0\n",
       "28871         0\n",
       "28876         0\n",
       "28878         0\n",
       "28879         0\n",
       "28881         0\n",
       "28882         0\n",
       "28889         0\n",
       "28893         0\n",
       "28895         0\n",
       "28896         0\n",
       "28897         0\n",
       "28898         0\n",
       "28899         0\n",
       "28900         0\n",
       "28903         0\n",
       "28905         0\n",
       "28906         0\n",
       "28916         0\n",
       "28922         0\n",
       "28930         0\n",
       "Name: Journal SJR, Length: 3032, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['Journal SJR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['Journal SJR'] = t['Journal SJR'].str.replace(',','.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40          NaN\n",
       "45          NaN\n",
       "66          NaN\n",
       "68        0.245\n",
       "83          NaN\n",
       "128         NaN\n",
       "137       1.601\n",
       "144       1.361\n",
       "147         NaN\n",
       "148       1.158\n",
       "149       1.158\n",
       "172      36.691\n",
       "176       6.141\n",
       "186       2.406\n",
       "221       0.755\n",
       "249       2.994\n",
       "252       1.608\n",
       "254       1.119\n",
       "261       1.633\n",
       "262       0.694\n",
       "269       2.163\n",
       "275       7.920\n",
       "276       1.649\n",
       "289       0.971\n",
       "291       3.280\n",
       "300       3.410\n",
       "318       1.185\n",
       "324         NaN\n",
       "334         NaN\n",
       "343       1.608\n",
       "          ...  \n",
       "28834       NaN\n",
       "28835       NaN\n",
       "28837       NaN\n",
       "28841       NaN\n",
       "28842       NaN\n",
       "28843       NaN\n",
       "28853       NaN\n",
       "28857       NaN\n",
       "28858       NaN\n",
       "28859       NaN\n",
       "28871       NaN\n",
       "28876       NaN\n",
       "28878       NaN\n",
       "28879       NaN\n",
       "28881       NaN\n",
       "28882       NaN\n",
       "28889       NaN\n",
       "28893       NaN\n",
       "28895       NaN\n",
       "28896       NaN\n",
       "28897       NaN\n",
       "28898       NaN\n",
       "28899       NaN\n",
       "28900       NaN\n",
       "28903       NaN\n",
       "28905       NaN\n",
       "28906       NaN\n",
       "28916       NaN\n",
       "28922       NaN\n",
       "28930       NaN\n",
       "Name: Journal SJR, Length: 3032, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['Journal SJR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40          NaN\n",
       "45          NaN\n",
       "66          NaN\n",
       "68        0.245\n",
       "83          NaN\n",
       "128         NaN\n",
       "137       1.601\n",
       "144       1.361\n",
       "147         NaN\n",
       "148       1.158\n",
       "149       1.158\n",
       "172      36.691\n",
       "176       6.141\n",
       "186       2.406\n",
       "221       0.755\n",
       "249       2.994\n",
       "252       1.608\n",
       "254       1.119\n",
       "261       1.633\n",
       "262       0.694\n",
       "269       2.163\n",
       "275       7.920\n",
       "276       1.649\n",
       "289       0.971\n",
       "291       3.280\n",
       "300       3.410\n",
       "318       1.185\n",
       "324         NaN\n",
       "334         NaN\n",
       "343       1.608\n",
       "          ...  \n",
       "28834       NaN\n",
       "28835       NaN\n",
       "28837       NaN\n",
       "28841       NaN\n",
       "28842       NaN\n",
       "28843       NaN\n",
       "28853       NaN\n",
       "28857       NaN\n",
       "28858       NaN\n",
       "28859       NaN\n",
       "28871       NaN\n",
       "28876       NaN\n",
       "28878       NaN\n",
       "28879       NaN\n",
       "28881       NaN\n",
       "28882       NaN\n",
       "28889       NaN\n",
       "28893       NaN\n",
       "28895       NaN\n",
       "28896       NaN\n",
       "28897       NaN\n",
       "28898       NaN\n",
       "28899       NaN\n",
       "28900       NaN\n",
       "28903       NaN\n",
       "28905       NaN\n",
       "28906       NaN\n",
       "28916       NaN\n",
       "28922       NaN\n",
       "28930       NaN\n",
       "Name: Journal SJR, Length: 3032, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['Journal SJR']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_excel('docs-filtered2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8653424648136232 -2.584962500721156\n"
     ]
    }
   ],
   "source": [
    "print(t['entropy'].median(),0.5*np.log2(1/36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>txt</th>\n",
       "      <th>source</th>\n",
       "      <th>tlen</th>\n",
       "      <th>processed</th>\n",
       "      <th>First topic</th>\n",
       "      <th>Second topic</th>\n",
       "      <th>First topic id</th>\n",
       "      <th>Second topic id</th>\n",
       "      <th>entropy</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1016/j.lfs.2020.117963</td>\n",
       "      <td>Virtual screening, ADME/Tox predictions and th...</td>\n",
       "      <td>virtual screening, adme/tox predictions and th...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>122</td>\n",
       "      <td>virtual screen adme_tox predict drug repurpos ...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>1.170804</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6381</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1080/07391102.2020.1775127</td>\n",
       "      <td>Identification of potential inhibitors of SARS...</td>\n",
       "      <td>identification of potential inhibitors of sars...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>165</td>\n",
       "      <td>identif potenti inhibitor sars_cov_2 endoribon...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>1.327902</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1111/febs.15369</td>\n",
       "      <td>Understanding SARS-CoV-2 endocytosis for COVID...</td>\n",
       "      <td>understanding sars-cov-2 endocytosis for covid...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>67</td>\n",
       "      <td>understand sars_cov_2 endocytosi covid_19 drug...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>cell,ace2,express,receptor,lung,immun,human,ge...</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>1.665919</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11590</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1096/fba.2020-00015</td>\n",
       "      <td>The role of growth factor receptors in viral i...</td>\n",
       "      <td>the role of growth factor receptors in viral i...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>142</td>\n",
       "      <td>role growth factor receptor viral infect oppor...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>cell,ace2,express,receptor,lung,immun,human,ge...</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>2.199779</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14772</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1038/s41586-020-2286-9</td>\n",
       "      <td>A SARS-CoV-2 protein interaction map reveals t...</td>\n",
       "      <td>a sars-cov-2 protein interaction map reveals t...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>74</td>\n",
       "      <td>sars_cov_2 protein interact map reveal target ...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>1.569275</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17791</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1016/j.ijantimicag.2020.105984</td>\n",
       "      <td>Combating devastating COVID-19 by drug repurpo...</td>\n",
       "      <td>combating devastating covid-19 by drug repurpo...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>51</td>\n",
       "      <td>combat devast covid_19 drug repurpos despit ad...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>trial,systemat,search,treatment,evid,includ,hy...</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>1.274839</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17904</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1016/j.drudis.2020.04.005</td>\n",
       "      <td>Boosting the arsenal against COVID-19 through ...</td>\n",
       "      <td>boosting the arsenal against covid-19 through ...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>77</td>\n",
       "      <td>boost arsenal covid_19 comput drug repurpos</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>research,articl,right,reserv,impact,protect,co...</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>2.521803</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1080/07391102.2020.1753577</td>\n",
       "      <td>Targeting SARS-CoV-2: a systematic drug repurp...</td>\n",
       "      <td>targeting sars-cov-2: a systematic drug repurp...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>155</td>\n",
       "      <td>target sars_cov_2 systemat drug repurpos appro...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>1.481687</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20436</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1080/07391102.2020.1752802</td>\n",
       "      <td>Computational studies of drug repurposing and ...</td>\n",
       "      <td>computational studies of drug repurposing and ...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>146</td>\n",
       "      <td>comput studi drug repurpos synerg lopinavir os...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>day,group,treatment,ventil,hospit,oxygen,treat...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>1.912257</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21798</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1038/d41587-020-00003-1</td>\n",
       "      <td>Coronavirus puts drug repurposing on the fast ...</td>\n",
       "      <td>coronavirus puts drug repurposing on the fast ...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>52</td>\n",
       "      <td>coronavirus put drug repurpos fast track</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>countri,outbreak,public,itali,respons,emerg,wo...</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>2.235374</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22068</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1038/s41421-020-0153-3</td>\n",
       "      <td>Network-based drug repurposing for novel coron...</td>\n",
       "      <td>network-based drug repurposing for novel coron...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>74</td>\n",
       "      <td>network_bas drug repurpos novel coronavirus hu...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>1.129385</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22069</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.12688/f1000research.22457.2</td>\n",
       "      <td>Prediction of the SARS-CoV-2 (2019-nCoV) 3C-li...</td>\n",
       "      <td>prediction of the sars-cov-2 (2019-ncov) 3c-li...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>170</td>\n",
       "      <td>predict sars_cov_2 proteas pro structur virtua...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>diabet,respir,n95,inactiv,heat,disinfect,repor...</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>1.163160</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23612</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1101/2020.06.16.154708</td>\n",
       "      <td>Targeting ACE2-RBD interaction as a platform f...</td>\n",
       "      <td>targeting ace2-rbd interaction as a platform f...</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>142</td>\n",
       "      <td>target ace2_rbd interact platform covid19 ther...</td>\n",
       "      <td>protein,bind,spike,structur,human,viral,sars_c...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>1.631306</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24169</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1101/2020.06.04.135046</td>\n",
       "      <td>An OpenData portal to share COVID-19 drug repu...</td>\n",
       "      <td>an opendata portal to share covid-19 drug repu...</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>71</td>\n",
       "      <td>opendata portal share covid_19 drug repurpos d...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>data,guidelin,inform,provid,intern,research,re...</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>1.656446</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24556</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1101/2020.05.27.20114371</td>\n",
       "      <td>Covid19db -- An online database of trials of m...</td>\n",
       "      <td>covid19db -- an online database of trials of m...</td>\n",
       "      <td>medrxiv</td>\n",
       "      <td>135</td>\n",
       "      <td>covid19db onlin databas trial medicin product ...</td>\n",
       "      <td>trial,systemat,search,treatment,evid,includ,hy...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>1.786979</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26923</th>\n",
       "      <td>2020</td>\n",
       "      <td>10.1101/2020.04.16.20068379</td>\n",
       "      <td>Prioritisation of potential anti-SARS-CoV-2 dr...</td>\n",
       "      <td>prioritisation of potential anti-sars-cov-2 dr...</td>\n",
       "      <td>medrxiv</td>\n",
       "      <td>192</td>\n",
       "      <td>prioritis potenti anti_sars_cov_2 drug repurpo...</td>\n",
       "      <td>transplant,liver,therapi,treatment,recipi,kidn...</td>\n",
       "      <td>drug,virus,vaccin,genom,viral,treatment,sequen...</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>1.826207</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       year                                doi  \\\n",
       "1106   2020          10.1016/j.lfs.2020.117963   \n",
       "6381   2020      10.1080/07391102.2020.1775127   \n",
       "8721   2020                 10.1111/febs.15369   \n",
       "11590  2020             10.1096/fba.2020-00015   \n",
       "14772  2020          10.1038/s41586-020-2286-9   \n",
       "17791  2020  10.1016/j.ijantimicag.2020.105984   \n",
       "17904  2020       10.1016/j.drudis.2020.04.005   \n",
       "20057  2020      10.1080/07391102.2020.1753577   \n",
       "20436  2020      10.1080/07391102.2020.1752802   \n",
       "21798  2020         10.1038/d41587-020-00003-1   \n",
       "22068  2020          10.1038/s41421-020-0153-3   \n",
       "22069  2020     10.12688/f1000research.22457.2   \n",
       "23612  2020          10.1101/2020.06.16.154708   \n",
       "24169  2020          10.1101/2020.06.04.135046   \n",
       "24556  2020        10.1101/2020.05.27.20114371   \n",
       "26923  2020        10.1101/2020.04.16.20068379   \n",
       "\n",
       "                                                   title  \\\n",
       "1106   Virtual screening, ADME/Tox predictions and th...   \n",
       "6381   Identification of potential inhibitors of SARS...   \n",
       "8721   Understanding SARS-CoV-2 endocytosis for COVID...   \n",
       "11590  The role of growth factor receptors in viral i...   \n",
       "14772  A SARS-CoV-2 protein interaction map reveals t...   \n",
       "17791  Combating devastating COVID-19 by drug repurpo...   \n",
       "17904  Boosting the arsenal against COVID-19 through ...   \n",
       "20057  Targeting SARS-CoV-2: a systematic drug repurp...   \n",
       "20436  Computational studies of drug repurposing and ...   \n",
       "21798  Coronavirus puts drug repurposing on the fast ...   \n",
       "22068  Network-based drug repurposing for novel coron...   \n",
       "22069  Prediction of the SARS-CoV-2 (2019-nCoV) 3C-li...   \n",
       "23612  Targeting ACE2-RBD interaction as a platform f...   \n",
       "24169  An OpenData portal to share COVID-19 drug repu...   \n",
       "24556  Covid19db -- An online database of trials of m...   \n",
       "26923  Prioritisation of potential anti-SARS-CoV-2 dr...   \n",
       "\n",
       "                                                     txt   source  tlen  \\\n",
       "1106   virtual screening, adme/tox predictions and th...   pubmed   122   \n",
       "6381   identification of potential inhibitors of sars...   pubmed   165   \n",
       "8721   understanding sars-cov-2 endocytosis for covid...   pubmed    67   \n",
       "11590  the role of growth factor receptors in viral i...   pubmed   142   \n",
       "14772  a sars-cov-2 protein interaction map reveals t...   pubmed    74   \n",
       "17791  combating devastating covid-19 by drug repurpo...   pubmed    51   \n",
       "17904  boosting the arsenal against covid-19 through ...   pubmed    77   \n",
       "20057  targeting sars-cov-2: a systematic drug repurp...   pubmed   155   \n",
       "20436  computational studies of drug repurposing and ...   pubmed   146   \n",
       "21798  coronavirus puts drug repurposing on the fast ...   pubmed    52   \n",
       "22068  network-based drug repurposing for novel coron...   pubmed    74   \n",
       "22069  prediction of the sars-cov-2 (2019-ncov) 3c-li...   pubmed   170   \n",
       "23612  targeting ace2-rbd interaction as a platform f...  biorxiv   142   \n",
       "24169  an opendata portal to share covid-19 drug repu...  biorxiv    71   \n",
       "24556  covid19db -- an online database of trials of m...  medrxiv   135   \n",
       "26923  prioritisation of potential anti-sars-cov-2 dr...  medrxiv   192   \n",
       "\n",
       "                                               processed  \\\n",
       "1106   virtual screen adme_tox predict drug repurpos ...   \n",
       "6381   identif potenti inhibitor sars_cov_2 endoribon...   \n",
       "8721   understand sars_cov_2 endocytosi covid_19 drug...   \n",
       "11590  role growth factor receptor viral infect oppor...   \n",
       "14772  sars_cov_2 protein interact map reveal target ...   \n",
       "17791  combat devast covid_19 drug repurpos despit ad...   \n",
       "17904        boost arsenal covid_19 comput drug repurpos   \n",
       "20057  target sars_cov_2 systemat drug repurpos appro...   \n",
       "20436  comput studi drug repurpos synerg lopinavir os...   \n",
       "21798           coronavirus put drug repurpos fast track   \n",
       "22068  network_bas drug repurpos novel coronavirus hu...   \n",
       "22069  predict sars_cov_2 proteas pro structur virtua...   \n",
       "23612  target ace2_rbd interact platform covid19 ther...   \n",
       "24169  opendata portal share covid_19 drug repurpos d...   \n",
       "24556  covid19db onlin databas trial medicin product ...   \n",
       "26923  prioritis potenti anti_sars_cov_2 drug repurpo...   \n",
       "\n",
       "                                             First topic  \\\n",
       "1106   protein,bind,spike,structur,human,viral,sars_c...   \n",
       "6381   protein,bind,spike,structur,human,viral,sars_c...   \n",
       "8721   drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "11590  drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "14772  drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "17791  drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "17904  drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "20057  protein,bind,spike,structur,human,viral,sars_c...   \n",
       "20436  protein,bind,spike,structur,human,viral,sars_c...   \n",
       "21798  drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "22068  drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "22069  protein,bind,spike,structur,human,viral,sars_c...   \n",
       "23612  protein,bind,spike,structur,human,viral,sars_c...   \n",
       "24169  drug,virus,vaccin,genom,viral,treatment,sequen...   \n",
       "24556  trial,systemat,search,treatment,evid,includ,hy...   \n",
       "26923  transplant,liver,therapi,treatment,recipi,kidn...   \n",
       "\n",
       "                                            Second topic  First topic id  \\\n",
       "1106   drug,virus,vaccin,genom,viral,treatment,sequen...              20   \n",
       "6381   drug,virus,vaccin,genom,viral,treatment,sequen...              20   \n",
       "8721   cell,ace2,express,receptor,lung,immun,human,ge...              27   \n",
       "11590  cell,ace2,express,receptor,lung,immun,human,ge...              27   \n",
       "14772  protein,bind,spike,structur,human,viral,sars_c...              27   \n",
       "17791  trial,systemat,search,treatment,evid,includ,hy...              27   \n",
       "17904  research,articl,right,reserv,impact,protect,co...              27   \n",
       "20057  drug,virus,vaccin,genom,viral,treatment,sequen...              20   \n",
       "20436  day,group,treatment,ventil,hospit,oxygen,treat...              20   \n",
       "21798  countri,outbreak,public,itali,respons,emerg,wo...              27   \n",
       "22068  protein,bind,spike,structur,human,viral,sars_c...              27   \n",
       "22069  diabet,respir,n95,inactiv,heat,disinfect,repor...              20   \n",
       "23612  drug,virus,vaccin,genom,viral,treatment,sequen...              20   \n",
       "24169  data,guidelin,inform,provid,intern,research,re...              27   \n",
       "24556  drug,virus,vaccin,genom,viral,treatment,sequen...               9   \n",
       "26923  drug,virus,vaccin,genom,viral,treatment,sequen...               8   \n",
       "\n",
       "       Second topic id   entropy annotation  \n",
       "1106                27  1.170804         NA  \n",
       "6381                27  1.327902         NA  \n",
       "8721                33  1.665919         NA  \n",
       "11590               33  2.199779         NA  \n",
       "14772               20  1.569275         NA  \n",
       "17791                9  1.274839         NA  \n",
       "17904               24  2.521803         NA  \n",
       "20057               27  1.481687         NA  \n",
       "20436               10  1.912257         NA  \n",
       "21798                5  2.235374         NA  \n",
       "22068               20  1.129385         NA  \n",
       "22069               18  1.163160         NA  \n",
       "23612               27  1.631306         NA  \n",
       "24169               29  1.656446         NA  \n",
       "24556               27  1.786979         NA  \n",
       "26923               27  1.826207         NA  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title'].str.match('.*drug repurposing.*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
